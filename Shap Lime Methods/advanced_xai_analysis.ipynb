{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Advanced Explainable AI (XAI) Analysis - 2025 Edition\n",
                "\n",
                "## Objective\n",
                "Beyond basic feature importance (SHAP/LIME), this notebook implements cutting-edge XAI techniques to answer deeper questions:\n",
                "1.  **Counterfactuals (DiCE)**: \"How do I *change* the outcome?\"\n",
                "2.  **Anchors (Alibi)**: \"What are the *sufficient conditions* for this prediction?\"\n",
                "3.  **Fairness (Fairlearn)**: \"Is the model *biased* against specific groups?\"\n",
                "4.  **Concept Bottlenecks**: \"Which high-level *concepts* (e.g., Service, Taste) drive the sentiment?\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install libraries if missed\n",
                "# !pip install dice-ml alibi fairlearn scikit-learn pandas numpy matplotlib seaborn sentence-transformers\n",
                "\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.cluster import KMeans\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Re-Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data from the backup created in the previous step\n",
                "try:\n",
                "    df = pd.read_csv('train_data_with_embeddings_BACKUP.csv')\n",
                "    print(\"Loaded data from backup.\")\n",
                "except FileNotFoundError:\n",
                "    print(\"Backup file not found. Loading original 'train_data.csv'...\")\n",
                "    df = pd.read_csv('train_data.csv')\n",
                "\n",
                "# --- DATA RECOVERY & VALIDATION ---\n",
                "# Check if 'embedding' or 'Sentiment' are missing and regenerate if needed\n",
                "\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "import ast\n",
                "\n",
                "# 1. Ensure Embeddings exist\n",
                "if 'embedding' not in df.columns:\n",
                "    print(\"'embedding' column missing. Generating embeddings now (this may take a minute)...\")\n",
                "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "    df['embedding'] = list(model.encode(df['review'].tolist(), show_progress_bar=True))\n",
                "else:\n",
                "    # If they exist but act like strings (from CSV), parse them\n",
                "    if isinstance(df['embedding'].iloc[0], str):\n",
                "        print(\"Parsing embedding strings to lists...\")\n",
                "        df['embedding'] = df['embedding'].apply(ast.literal_eval)\n",
                "\n",
                "# 2. Ensure Sentiment exists\n",
                "if 'Sentiment' not in df.columns:\n",
                "    print(\"'Sentiment' column missing. Deriving sentiment from embeddings...\")\n",
                "    # Initialize model if not already done\n",
                "    if 'model' not in locals():\n",
                "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "        \n",
                "    pos_anchor = model.encode([\"Positive restaurant experience, delicious food, great service, loved it\"])\n",
                "    neg_anchor = model.encode([\"Negative restaurant experience, bad food, terrible service, not recommended\"])\n",
                "    \n",
                "    def get_sentiment(emb):\n",
                "        # Ensure emb is array\n",
                "        emb = np.array(emb).reshape(1, -1)\n",
                "        pos_sim = cosine_similarity(emb, pos_anchor)[0][0]\n",
                "        neg_sim = cosine_similarity(emb, neg_anchor)[0][0]\n",
                "        return 1 if pos_sim > neg_sim else 0\n",
                "        \n",
                "    df['Sentiment'] = df['embedding'].apply(get_sentiment)\n",
                "    print(\"Sentiment derived successfully.\")\n",
                "\n",
                "print(\"Data Validation Complete.\")\n",
                "# ----------------------------------\n",
                "\n",
                "# Preprocessing\n",
                "if 'Gender_Code' not in df.columns:\n",
                "    le_gender = LabelEncoder()\n",
                "    df['Gender_Code'] = le_gender.fit_transform(df['gender'])\n",
                "    \n",
                "if 'Meal_Code' not in df.columns:\n",
                "    le_meal = LabelEncoder()\n",
                "    df['Meal_Code'] = le_meal.fit_transform(df['meal_category'])\n",
                "\n",
                "feature_names = ['age', 'Gender_Code', 'Meal_Code']\n",
                "target_name = 'Sentiment'\n",
                "\n",
                "X = df[feature_names]\n",
                "y = df[target_name]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "clf.fit(X_train, y_train)\n",
                "print(f\"Model Accuracy: {clf.score(X_test, y_test):.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Counterfactual Explanations (DiCE)\n",
                "Generates \"What-if\" examples to flip the prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import dice_ml\n",
                "from dice_ml.utils import helpers  # helper functions\n",
                "\n",
                "# DiCE requires a specific data object\n",
                "# We handle numeric 'Gender_Code' and 'Meal_Code' as discrete categorical, or continuous for simplicity?\n",
                "# Usually DiCE wants readable names. Let's make a readable dataframe for exploration\n",
                "df_dice = df[feature_names + [target_name]].copy()\n",
                "\n",
                "d = dice_ml.Data(dataframe=df_dice, \n",
                "                 continuous_features=['age'], \n",
                "                 outcome_name=target_name)\n",
                "\n",
                "# Using 'sklearn' backend for RandomForest\n",
                "m = dice_ml.Model(model=clf, backend=\"sklearn\")\n",
                "\n",
                "# Initialize DiCE\n",
                "exp = dice_ml.Dice(d, m, method=\"random\")\n",
                "\n",
                "# Query: Show me how to change a Negative prediction (0) to Positive (1)\n",
                "# Let's pick a negative instance from test set\n",
                "negative_instances = X_test[y_test == 0]\n",
                "if len(negative_instances) > 0:\n",
                "    query_instance = negative_instances.iloc[[0]]\n",
                "    \n",
                "    print(\"Query Instance (Negative Prediction):\")\n",
                "    print(query_instance)\n",
                "    \n",
                "    # Generate Counterfactuals\n",
                "    # Generating 3 examples with 'opposite' outcome\n",
                "    dice_exp = exp.generate_counterfactuals(query_instance, total_CFs=3, desired_class=\"opposite\")\n",
                "    \n",
                "    # Visualize\n",
                "    dice_exp.visualize_as_dataframe(show_only_changes=True)\n",
                "else:\n",
                "    print(\"No negative instances found in test set to explain.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Anchor Explanations (Alibi)\n",
                "Finds high-precision rules (sufficient conditions) for a prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from alibi.explainers import AnchorTabular\n",
                "\n",
                "# Initialize Anchor explainer\n",
                "predict_fn = lambda x: clf.predict(x)\n",
                "explainer = AnchorTabular(predict_fn, feature_names)\n",
                "\n",
                "# Determine feature categorical map for Alibi (it needs to know which cols are categorical)\n",
                "# Age is continuous (roughly), Gender/Meal are categorical\n",
                "# We used LabelEncoder 0,1,2...\n",
                "explainer.fit(X_train.values, categorical_names={1: ['Female', 'Male'], 2: ['Asian', 'Main', 'Snack', 'etc']})\n",
                "\n",
                "# Explain an instance\n",
                "idx = 0\n",
                "explanation = explainer.explain(X_test.iloc[idx].values, threshold=0.95)\n",
                "\n",
                "print(f\"Anchor Explanation for Instance {idx}:\")\n",
                "print(f\"Prediction: {predict_fn([X_test.iloc[idx].values])[0]}\")\n",
                "print(f\"Anchor Rule: {explanation.anchor}\")\n",
                "print(f\"Precision: {explanation.precision:.2f}\")\n",
                "print(f\"Coverage: {explanation.coverage:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Fairness & Bias Auditing (Fairlearn)\n",
                "Checks if the model treats demographic groups equally."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from fairlearn.metrics import MetricFrame, selection_rate, count\n",
                "from fairlearn.plotting import plot_box_plots\n",
                "\n",
                "# Define sensitive feature (Gender)\n",
                "sensitive_feature = X_test['Gender_Code']\n",
                "\n",
                "# Calculate metrics\n",
                "metrics = {\n",
                "    'accuracy': accuracy_score,\n",
                "    'selection_rate': selection_rate, # How often model predicts 'Positive'\n",
                "    'count': count\n",
                "}\n",
                "\n",
                "metric_frame = MetricFrame(\n",
                "    metrics=metrics,\n",
                "    y_true=y_test,\n",
                "    y_pred=clf.predict(X_test),\n",
                "    sensitive_features=sensitive_feature\n",
                ")\n",
                "\n",
                "print(\"Fairness Metrics by Group (0/1 Gender):\")\n",
                "print(metric_frame.by_group)\n",
                "\n",
                "# Visualizing Selection Rate (Demographic Parity)\n",
                "metric_frame.by_group['selection_rate'].plot(kind='bar', title=\"Selection Rate by Gender\")\n",
                "plt.ylabel(\"Fraction Predicted Positive\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Concept Bottleneck Model (Post-hoc Concept Discovery)\n",
                "Clustering embeddings to find high-level \"Concepts\" and measuring their impact on Sentiment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Parse Embeddings from String (if loaded from CSV they might be strings)\n",
                "# If using backup, they are usually stored as string representations of lists\n",
                "import ast\n",
                "\n",
                "# Check format - we already did this in setup, but doing it safely again\n",
                "# If we just regenerated, it's list. If from CSV previously, it might be string\n",
                "# But Setup cell handles it now.\n",
                "\n",
                "X_emb = np.stack(df['embedding'].values)\n",
                "\n",
                "# 2. Cluster Embeddings into 'Concepts' (e.g., K=5)\n",
                "n_concepts = 5\n",
                "kmeans = KMeans(n_clusters=n_concepts, random_state=42)\n",
                "concept_labels = kmeans.fit_predict(X_emb)\n",
                "df['Concept_Cluster'] = concept_labels\n",
                "\n",
                "# 3. Interpret Concepts (Look at top keywords/reviews in each cluster)\n",
                "print(\"--- Discovered Concepts ---\")\n",
                "for i in range(n_concepts):\n",
                "    print(f\"\\nConcept {i} Sample Reviews:\")\n",
                "    print(df[df['Concept_Cluster'] == i]['review'].head(3).values)\n",
                "\n",
                "# 4. Concept Importance (Linear Model: Concept -> Sentiment)\n",
                "# We One-Hot Encode concepts\n",
                "concepts_oh = pd.get_dummies(df['Concept_Cluster'], prefix='Concept')\n",
                "\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "concept_model = LogisticRegression()\n",
                "concept_model.fit(concepts_oh, df['Sentiment'])\n",
                "\n",
                "print(\"\\n--- Concept Importance (Impact on Sentiment) ---\")\n",
                "coefs = concept_model.coef_[0]\n",
                "for i, coef in enumerate(coefs):\n",
                "    impact = \"Positive\" if coef > 0 else \"Negative\"\n",
                "    print(f\"Concept {i}: Weight {coef:.2f} ({impact})\")\n",
                "\n",
                "plt.figure(figsize=(8, 4))\n",
                "plt.bar(range(n_concepts), coefs)\n",
                "plt.title(\"Impact of Discovered Concepts on Sentiment\")\n",
                "plt.xlabel(\"Concept Cluster\")\n",
                "plt.ylabel(\"Regression Coefficient\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
